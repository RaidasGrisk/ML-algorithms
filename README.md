# Things implemented simply

The idea is to make everything simple and readable. Take a concept and implement it from scratch using simple math (numpy in this case). If you manage to do that, hopefully, that will help you to understand a thing or two about it.

1. [**Multivariate regression**](/Multivariate_regression.py). Basic and simple implementation of multivariate regression. Optimized using gradient decent with regularization.

2. [**Logistic regression**](/Logistic_regression.py). The same as before. Simple logistic regression. 

3. [**Simple neural network**](/Neural_network_v2.py). Simple neural net. Optimized using dropout and Adam algorithm.

1, 2 and 3 alogorithms are based on Andrew's NG machine learning course on Coursera (https://www.coursera.org/learn/machine-learning).


# ML-algorithms in practice

Few examples of algorithms I've tried with while learning ML.

1. [**Policy gradient**](/Policy_gradient_breakout.py). First take at reinforcement learning. Basic policy gradient implementation using tensorflow and Breakout from openAI's gym. I tried to keep it as simple as possible with only minor exceptions. Hopefully, this is easy to read and understand.

2. [**Deep Q Network (this one is not finished, still working on it)**](/DQN_Breakout.py). Second take at reinforcement learning. Basic DQN implementation using tensorflow and Breakout from openAI's gym.






3. [**Simple neural network**](/Neural_network.py). Simple (yet terribly coded) neural network implemented using numpy only. This was my first take at trying to understand neural nets. No other functionalities (e.g dropout), except for regularization and variable hidden layers and neurons. At the time I wrote this I had rather shitty programming skills, so reading and trying to understand this code it quite agonizing (be warned).

